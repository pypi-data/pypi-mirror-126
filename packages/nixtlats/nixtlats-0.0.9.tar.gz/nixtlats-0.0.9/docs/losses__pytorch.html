---

title: PyTorch losses


keywords: fastai
sidebar: home_sidebar

summary: "Training losses."
description: "Training losses."
nb_path: "nbs/losses__pytorch.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/losses__pytorch.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="divide_no_nan" class="doc_header"><code>divide_no_nan</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L11" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>divide_no_nan</code>(<strong><code>a</code></strong>, <strong><code>b</code></strong>)</p>
</blockquote>
<p>Auxiliary funtion to handle divide by 0</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PyTorch-Train-Losses">PyTorch Train Losses<a class="anchor-link" href="#PyTorch-Train-Losses"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MAPELoss" class="doc_header"><code>MAPELoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L21" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MAPELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>MAPE Loss</p>
<p>Calculates Mean Absolute Percentage Error between
y and y_hat. MAPE measures the relative prediction
accuracy of a forecasting method by calculating the
percentual deviation of the prediction and the true
value at a given time and averages these devations
over the length of the series.
As defined in: <a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">https://en.wikipedia.org/wiki/Mean_absolute_percentage_error</a></p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
mask: tensor (batch_size, output_size)
    specifies date stamps per serie
    to consider in loss</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>mape:
Mean absolute percentage error.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MSELoss" class="doc_header"><code>MSELoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L55" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MSELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>MSE Loss</p>
<p>Calculates Mean Squared Error between
y and y_hat. MAPE measures the relative prediction
accuracy of a forecasting method by calculating the
percentual deviation of the prediction and the true
value at a given time and averages these devations
over the length of the series.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
mask: tensor (batch_size, output_size)
    specifies date stamps per serie
    to consider in loss</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>mse:
Mean Squared Error.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="RMSELoss" class="doc_header"><code>RMSELoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L88" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>RMSELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>RMSE Loss</p>
<p>Calculates Mean Squared Error between
y and y_hat. MAPE measures the relative prediction
accuracy of a forecasting method by calculating the
percentual deviation of the prediction and the true
value at a given time and averages these devations
over the length of the series.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
mask: tensor (batch_size, output_size)
    specifies date stamps per serie
    to consider in loss</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>rmse:
Root Mean Squared Error.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="SMAPELoss" class="doc_header"><code>SMAPELoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L121" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>SMAPELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>SMAPE2 Loss</p>
<p>Calculates Symmetric Mean Absolute Percentage Error.
SMAPE measures the relative prediction accuracy of a
forecasting method by calculating the relative deviation
of the prediction and the true value scaled by the sum of the
absolute values for the prediction and true value at a
given time, then averages these devations over the length
of the series. This allows the SMAPE to have bounds between
0% and 200% which is desireble compared to normal MAPE that
may be undetermined.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>smape:
    symmetric mean absolute percentage error</p>
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2><p>[1] <a href="https://robjhyndman.com/hyndsight/smape/">https://robjhyndman.com/hyndsight/smape/</a> (Makridakis 1993)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MASELoss" class="doc_header"><code>MASELoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L160" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MASELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>y_insample</code></strong>, <strong><code>seasonality</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates the M4 Mean Absolute Scaled Error.</p>
<p>MASE measures the relative prediction accuracy of a
forecasting method by comparinng the mean absolute errors
of the prediction and the true value against the mean
absolute errors of the seasonal naive model.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>seasonality: int
    main frequency of the time series
    Hourly 24,  Daily 7, Weekly 52,
    Monthly 12, Quarterly 4, Yearly 1
y: tensor (batch_size, output_size)
    actual test values
y_hat: tensor (batch_size, output_size)
    predicted values
y_train: tensor (batch_size, input_size)
    actual insample values for Seasonal Naive predictions</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>mase:
    mean absolute scaled error</p>
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2><p>[1] <a href="https://robjhyndman.com/papers/mase.pdf">https://robjhyndman.com/papers/mase.pdf</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MAELoss" class="doc_header"><code>MAELoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L201" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MAELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>MAE Loss</p>
<p>Calculates Mean Absolute Error between
y and y_hat. MAE measures the relative prediction
accuracy of a forecasting method by calculating the
deviation of the prediction and the true
value at a given time and averages these devations
over the length of the series.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
mask: tensor (batch_size, output_size)
    specifies date stamps per serie
    to consider in loss</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>mae:
Mean absolute error.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="PinballLoss" class="doc_header"><code>PinballLoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L233" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>PinballLoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>, <strong><code>tau</code></strong>=<em><code>0.5</code></em>)</p>
</blockquote>
<p>Pinball Loss
Computes the pinball loss between y and y_hat.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
tau: float, between 0 and 1
    the slope of the pinball loss, in the context of
    quantile regression, the value of tau determines the
    conditional quantile level.</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>pinball:
    average accuracy for the predicted quantile</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="ES-RNN-PyTorch-loss">ES-RNN PyTorch loss<a class="anchor-link" href="#ES-RNN-PyTorch-loss"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LevelVariabilityLoss" class="doc_header"><code>LevelVariabilityLoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L262" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LevelVariabilityLoss</code>(<strong><code>levels</code></strong>, <strong><code>level_variability_penalty</code></strong>)</p>
</blockquote>
<p>Level Variability Loss
Computes the variability penalty for the level.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>levels: tensor with shape (batch, n_time)
    levels obtained from exponential smoothing component of ESRNN
level_variability_penalty: float
    this parameter controls the strength of the penalization
    to the wigglines of the level vector, induces smoothness
    in the output</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>level_var_loss:
    wiggliness loss for the level vector</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="SmylLoss" class="doc_header"><code>SmylLoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L294" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>SmylLoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>levels</code></strong>, <strong><code>mask</code></strong>, <strong><code>tau</code></strong>, <strong><code>level_variability_penalty</code></strong>=<em><code>0.0</code></em>)</p>
</blockquote>
<p>Computes the Smyl Loss that combines level variability with
with Pinball loss.
windows_y: tensor of actual values,
                        shape (n_windows, batch_size, window_size).
windows_y_hat: tensor of predicted values,
                                shape (n_windows, batch_size, window_size).
levels: levels obtained from exponential smoothing component of ESRNN.
                tensor with shape (batch, n_time).
return: smyl_loss.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Multi-quantile-PyTorch-loss">Multi-quantile PyTorch loss<a class="anchor-link" href="#Multi-quantile-PyTorch-loss"> </a></h1><p>MQLoss definition and testing</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MQLoss" class="doc_header"><code>MQLoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L317" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MQLoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>quantiles</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>MQLoss</p>
<p>Calculates Average Multi-quantile Loss function, for
a given set of quantiles, based on the absolute
difference between predicted and true values.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>y: tensor (batch_size, output_size) actual values in torch tensor.
y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.
mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie
      to consider in loss
quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>lq: tensor(n_quantiles) average multi-quantile loss.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="wMQLoss" class="doc_header"><code>wMQLoss</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/losses/pytorch.py#L350" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>wMQLoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>quantiles</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>wMQLoss</p>
<p>Calculates Average Multi-quantile Loss function, for
a given set of quantiles, based on the absolute
difference between predicted and true values.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>y: tensor (batch_size, output_size) actual values in torch tensor.
y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.
mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie
      to consider in loss
quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>lq: tensor(n_quantiles) average multi-quantile loss.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Checks-for-PyTorch-train-losses">Checks for PyTorch train losses<a class="anchor-link" href="#Checks-for-PyTorch-train-losses"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">hmean</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">horizon</span><span class="p">,</span> <span class="n">n_quantiles</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">horizon</span> <span class="o">=</span> <span class="n">horizon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span> <span class="o">=</span> <span class="n">n_quantiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">n_obs</span><span class="p">,</span> 
                                      <span class="n">out_features</span><span class="o">=</span><span class="n">horizon</span> <span class="o">*</span> <span class="n">n_quantiles</span><span class="p">,</span> 
                                      <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">horizon</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>
    
<span class="k">class</span> <span class="nc">Data</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    
    <span class="c1"># Constructor</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Getter</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    
    <span class="c1"># Get Length</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">len</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>

<span class="c1"># Sample data</span>
<span class="n">n_ts</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_obs</span> <span class="o">=</span> <span class="n">horizon</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># to generate random numbers from N(mean, std)</span>
<span class="n">std</span> <span class="o">=</span> <span class="mf">7.0</span> <span class="c1"># to generate random numbers from N(mean, std)</span>
<span class="n">start</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="c1"># First quantile</span>
<span class="n">end</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="c1"># Last quantiles</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Number of quantiles</span>

<span class="c1"># Hyperparameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.08</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Sample data</span>
<span class="n">quantiles</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0500</span><span class="p">,</span> <span class="mf">0.3500</span><span class="p">,</span> <span class="mf">0.6500</span><span class="p">,</span> <span class="mf">0.9500</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;quantiles:</span><span class="se">\n</span><span class="si">{</span><span class="n">quantiles</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_ts</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_ts</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">))</span>

<span class="n">Y_test</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_ts</span><span class="p">,</span> <span class="n">horizon</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_ts</span><span class="p">,</span> <span class="n">horizon</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Y.shape: </span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, X.shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Y_test.shape: </span><span class="si">{</span><span class="n">Y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, X_test.shape: </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>quantiles:
tensor([0.0500, 0.3500, 0.6500, 0.9500])
Y.shape: torch.Size([1000, 10]), X.shape: torch.Size([1000, 10])
Y_test.shape: torch.Size([1000, 10]), X_test.shape: torch.Size([1000, 10])
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">horizon</span><span class="o">=</span><span class="n">horizon</span><span class="p">,</span> <span class="n">n_quantiles</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">Data</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">print_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="n">training_trajectory</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="p">[],</span>
                           <span class="s1">&#39;train_loss&#39;</span><span class="p">:</span> <span class="p">[]}</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1">#training_loss = wMQLoss(y=y, y_hat=y_hat, quantiles=quantiles)</span>
            <span class="n">training_loss</span> <span class="o">=</span> <span class="n">MQLoss</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="o">=</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="n">quantiles</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
                <span class="n">training_trajectory</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">training_trajectory</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">training_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">display_string</span> <span class="o">=</span> <span class="s1">&#39;Step: </span><span class="si">{}</span><span class="s1">, Time: </span><span class="si">{:03.3f}</span><span class="s1">, Insample </span><span class="si">{}</span><span class="s1">: </span><span class="si">{:.5f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> 
                                                                                    <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span> 
                                                                                    <span class="s2">&quot;MQLoss&quot;</span><span class="p">,</span> 
                                                                                    <span class="n">training_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">print_progress</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">display_string</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">training_trajectory</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

